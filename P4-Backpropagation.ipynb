{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.error = float\n",
    "        self.delta_weights = []\n",
    "        self.delta_bias = 0\n",
    "        self.lr = 0.1\n",
    "    \n",
    "    def calculate_neuron(self, inputs):\n",
    "        \"\"\"\n",
    "        Calculate the output of this neuron\n",
    "    \n",
    "        \"\"\"\n",
    "        list_outputs = []\n",
    "        if len(inputs) == len(self.weights):\n",
    "            for x, weight in zip(inputs, self.weights):\n",
    "                list_outputs.append(x*weight)\n",
    "        else:\n",
    "            raise ValueError(\"The list of inputs and weights must have the same length\")\n",
    "        \n",
    "        self.sum_output = sum(list_outputs)\n",
    "        outcome = self.sum_output + self.bias\n",
    "        output_sigmoid = self.sigmoid_function(outcome)\n",
    "        return output_sigmoid\n",
    "    \n",
    "    def sigmoid_function(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def error_output_neuron(self, output_neuron, target):\n",
    "        self.error = output_neuron * (1 - output_neuron) * - (target - output_neuron)\n",
    "    \n",
    "    def error_hidden_neuron(sum_output_error_next_layer):\n",
    "        self.error = sum_output_error_next_layer * sum_weights * delta_next_layer\n",
    "            \n",
    "    def gradient_x(self, output, delta):\n",
    "        return output * delta\n",
    "    \n",
    "    def set_delta_weights_bias(self, lr, output, delta):\n",
    "        self.delta_weights.append(lr * self.gradient_x(output, delta))\n",
    "        self.delta_bias = lr * delta #TODO iedere x dat je weight assigned overschrijf je de bias onnodig\n",
    "        \n",
    "    def calculate_error_outputneuron(self, output_neural_network, target):\n",
    "        lr = 0.1 #TODO waar komt learning rate te staan? --> naar init\n",
    "        self.error_output_neuron(output_neural_network, target)\n",
    "        self.set_delta_weights_bias(lr, output_neural_network, self.error)\n",
    "        \n",
    "    def backpropagation(self):\n",
    "        for weight in self.weights:\n",
    "            self.delta_weights.append(weight*self.error) # delta weight is lr * output --> \n",
    "        self.delta_bias = self.lr * self.error\n",
    "        \n",
    "    def update_neuron(self):\n",
    "        for weight_number in range(len(self.weights)):\n",
    "            self.weights[weight_number] -= self.delta_weights[weight_number]\n",
    "        \n",
    "        self.bias-= self.delta_bias\n",
    "        \n",
    "        self.delta_weights = []\n",
    "           \n",
    "    def __str__(self):\n",
    "        return f'Weights are {self.weights} and bias is {self.bias}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Neuron_layer:\n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons\n",
    "    \n",
    "    def calculate_layer(self, inputs):\n",
    "        output = []\n",
    "        for neuron in self.neurons:\n",
    "            output.append(neuron.calculate_neuron(inputs))\n",
    "        return output\n",
    "    \n",
    "    def set_error_output_layer(self, outputNN, target):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.calculate_error_outputneuron(outputNN, target) # --> let op outputneuron functie wordt hier aangeroepen. -> check dit\n",
    "    \n",
    "    def update_layer(self):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.update_neuron()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'The layer consist out of these neurons: {self.neurons}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Neuron_network:\n",
    "    def __init__(self, neuron_layers):\n",
    "        self.neuron_layers = neuron_layers\n",
    "    \n",
    "    def feed_forward(self, inputs):\n",
    "        for neuron_layer in self.neuron_layers:\n",
    "            inputs = neuron_layer.calculate_layer(inputs)\n",
    "        return inputs\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'The network consist out of these perceptron_layers: {self.neuron_layers}'\n",
    "    \n",
    "    def MSE(self, target_list, output_list): # loss function \n",
    "        print(target_list, output_list)\n",
    "        return np.sum([(a-b)**2 for a,b in zip(target_list, output_list)])\n",
    "    \n",
    "    def train_nn(self, posible_inputs):\n",
    "        \"\"\"\n",
    "        Put in like this [[[0, 0], 0],\n",
    "                          [[0, 1], 0],\n",
    "                          [[1, 0], 0],\n",
    "                          [[1, 1], 1]]\n",
    "        \"\"\"\n",
    "        print('x1 x2 target output')\n",
    "        error = 1 # The error is unknown but will be assigned after the first check\n",
    "        epoch = 0\n",
    "        while epoch < 10: #error < -0.1 or error > 0.1:\n",
    "            li = []\n",
    "            for posible_input in posible_inputs:  \n",
    "                outputNN = self.feed_forward(posible_input[0])[0]\n",
    "                li.append(outputNN)\n",
    "            \n",
    "                # sets delta of weights and bias of outputlayer\n",
    "                print('________')\n",
    "                print(outputNN, posible_input[1])\n",
    "                print('__________')\n",
    "            \n",
    "                # backpropagation\n",
    "                \n",
    "                self.neuron_layers[-1].set_error_output_layer(outputNN, posible_input[1])\n",
    "\n",
    "                \n",
    "                for left_layer, right_layer in zip(self.neuron_layers[::-1], self.neuron_layers[-2::-1]):\n",
    "                    for neuron in right_layer.self.neurons:\n",
    "                        for index in len(right_layer.self.neurons.self.weights):\n",
    "                            left_layer.neurons[index].error += right_layer.self.neurons.self.weights[index] * right_layer.self.neurons[index].self.error\n",
    "                        for index in len(right_layer.self.neurons.self.weights):\n",
    "                            left_layer.neurons[index].backpropagation()\n",
    "                            \n",
    "                # update     \n",
    "                for layer in self.neuron_layers:\n",
    "                    layer.update_layer()            \n",
    "            error = self.MSE([x[1] for x in posible_inputs], li)\n",
    "            print('error = ', error)\n",
    "            if error > -0.1 and error < 0.1:\n",
    "                print('network trained!')\n",
    "            \n",
    "            epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND-poort\n",
    "\n",
    "and_neuron = Neuron([1, 1], 1)\n",
    "\n",
    "output_layer = Neuron_layer([and_neuron])\n",
    "\n",
    "neural_network_AND = Neuron_network([output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 x2 target output\n",
      "________\n",
      "0.7310585786300049 0\n",
      "__________\n",
      "[1, 1]\n",
      "[0.010507858816426222]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-bc21ce9931be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                              \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                              \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                              [[1, 1], 1]])\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-7f68baa5dd0b>\u001b[0m in \u001b[0;36mtrain_nn\u001b[1;34m(self, posible_inputs)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;31m# update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneuron_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                     \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mposible_inputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mli\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error = '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-2c968dcecec3>\u001b[0m in \u001b[0;36mupdate_layer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mneuron\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mneuron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_neuron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-77a28702b5f7>\u001b[0m in \u001b[0;36mupdate_neuron\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mweight_number\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mweight_number\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mweight_number\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "neural_network_AND.train_nn([[[0, 0], 0],\n",
    "                             [[0, 1], 0],\n",
    "                             [[1, 0], 0],\n",
    "                             [[1, 1], 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[0,0],[0,1],[1,0],[1,1]]\n",
    "\n",
    "print('x1, x2, carry, sum')\n",
    "for input_x in inputs:\n",
    "    print(input_x, '   ', neural_network_AND.feed_forward(input_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR-poort\n",
    "\n",
    "xor_neuron = Neuron([1 , 1], 1)\n",
    "\n",
    "output_layer = Neuron_layer([xor_neuron])\n",
    "\n",
    "neural_network_XOR = Neuron_network([output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training XOR network\n",
    "neural_network_XOR.train_nn([[[0, 0], 0],\n",
    "                             [[0, 1], 1],\n",
    "                             [[1, 0], 1],\n",
    "                             [[1, 1], 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR_network activate\n",
    "\n",
    "inputs = [[0,0],[0,1],[1,0],[1,1]]\n",
    "\n",
    "print('x1, x2, NOR_port')\n",
    "for input_x in inputs:\n",
    "    print(input_x, '   ', neural_network_XOR.feed_forward(input_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half adder\n",
    "hidden_neuron_NAND = Neuron([1, 1], 1)\n",
    "hidden_neuron_OR = Neuron([1, 1], 1)\n",
    "hidden_neuron_AND = Neuron([1, 1], 1)\n",
    "\n",
    "output_neuron_sum = Neuron([1, 1, 1], 1) # AND-poort\n",
    "output_neuron_carry = Neuron([1, 1, 1], 1) \n",
    "\n",
    "hidden_neuron_layer = Neuron_layer([hidden_neuron_NAND, hidden_neuron_OR, hidden_neuron_AND])\n",
    "output_neuron_layer = Neuron_layer([output_neuron_carry, output_neuron_sum])\n",
    "\n",
    "neural_network_neurons = Neuron_network([hidden_neuron_layer, output_neuron_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training XOR network\n",
    "neural_network_AND.train_nn([[[0, 0], [0,0]],\n",
    "                             [[0, 1], [0,1]],\n",
    "                             [[1, 0], [0,1]],\n",
    "                             [[1, 1], [1,0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bij sigmoid gaan we er vanuit de het omslagpunt altijd op de nul ligt. Daarom kun je dat omslagpunt niet veranderen. Dan werkt de sigmoid functie niet meer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half adder activate\n",
    "\n",
    "inputs = [[0,0],[0,1],[1,0],[1,1]]\n",
    "\n",
    "print('x1, x2, carry, sum')\n",
    "for input_x in inputs:\n",
    "    print(input_x, '   ', neural_network_neurons.feed_forward(input_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit6aefb534615b451c8b9f779fc82cec76"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
