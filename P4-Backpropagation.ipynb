{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.error = float\n",
    "        self.delta_weights = []\n",
    "        self.delta_bias = 0\n",
    "    \n",
    "    def calculate_neuron(self, inputs):\n",
    "        \"\"\"\n",
    "        Calculate the output of this neuron\n",
    "    \n",
    "        \"\"\"\n",
    "        list_outputs = []\n",
    "        if len(inputs) == len(self.weights):\n",
    "            for x, weight in zip(inputs, self.weights):\n",
    "                list_outputs.append(x*weight)\n",
    "        else:\n",
    "            raise ValueError(\"The list of inputs and weights must have the same length\")\n",
    "         \n",
    "        outcome = sum(list_outputs) + self.bias\n",
    "        output_sigmoid = self.sigmoid_function(outcome)\n",
    "        return output_sigmoid\n",
    "    \n",
    "    def sigmoid_function(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def error_output_neuron(self, output_neuron, target):\n",
    "        self.error = output_neuron * (1-output_neuron) * - (target - output_neuron)\n",
    "            \n",
    "    def gradient_x(self, output, delta):\n",
    "        return output * delta\n",
    "    \n",
    "    def set_delta_weights_bias(self, lr, output, delta):\n",
    "        self.delta_weight = lr * self.gradient_x(output, delta)\n",
    "        self.delta_bias = lr * delta\n",
    "        \n",
    "    def calculate_error_outputneuron(self, output_neural_network, target):\n",
    "        lr = 0.1 #TODO waar komt learning rate te staan?\n",
    "        self.error_output_neuron(output_neural_network, target)\n",
    "        self.set_delta_weights_bias(lr, output_neural_network, self.error)\n",
    "        \n",
    "    def backpropagation(self):\n",
    "        for weight in enumerate(self.weights):\n",
    "            self.weight_delta.append = weight*self.error\n",
    "        self.bias = self.bias * self.error\n",
    "        \n",
    "    def update_neuron(self):\n",
    "        print(self.weights)\n",
    "        print(self.delta_weights)\n",
    "        for weight_number in range(len(self.weights)):\n",
    "            self.weights[weight_number] -= self.delta_weights[weight_number]\n",
    "        \n",
    "        self.bias-= self.delta_bias\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'Weights are {self.weights} and bias is {self.bias}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Neuron_layer:\n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons\n",
    "    \n",
    "    def calculate_layer(self, inputs):\n",
    "        output = []\n",
    "        for neuron in self.neurons:\n",
    "            output.append(neuron.calculate_neuron(inputs))\n",
    "        return output\n",
    "    \n",
    "    def set_error(self, outputNN, target):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.calculate_error_outputneuron(outputNN, target)\n",
    "    \n",
    "    def update_layer(self):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.update_neuron()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'The layer consist out of these neurons: {self.neurons}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Neuron_network:\n",
    "    def __init__(self, neuron_layers):\n",
    "        self.neuron_layers = neuron_layers\n",
    "    \n",
    "    def feed_forward(self, inputs):\n",
    "        for neuron_layer in self.neuron_layers:\n",
    "            inputs = neuron_layer.calculate_layer(inputs)\n",
    "        return inputs\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'The network consist out of these perceptron_layers: {self.neuron_layers}'\n",
    "    \n",
    "    def train_nn(self, posible_inputs):\n",
    "        \"\"\"\n",
    "        Put in like this [[[0, 0], 0],\n",
    "                          [[0, 1], 0],\n",
    "                          [[1, 0], 0],\n",
    "                          [[1, 1], 1]]\n",
    "        \"\"\"\n",
    "        \n",
    "        outcome = 0\n",
    "        print('x1 x2 target output')\n",
    "        error = 1 # The error is unknown but will be assigned after the first check\n",
    "        while error != 0:\n",
    "            li = []\n",
    "            for posible_input in posible_inputs:\n",
    "                    \n",
    "                outputNN = self.feed_forward(posible_input[0])[0]\n",
    "                \n",
    "                li.append(outputNN)\n",
    "                #last_layer = self.neuron_layers[-1]\n",
    "            \n",
    "                # sets delta of weights and bias of outputlayer\n",
    "                self.neuron_layers[-1].set_error(outputNN, posible_input[1])\n",
    "                \n",
    "                #for neuron in self.neuron_layers[-1].self.neurons:\n",
    "                    #neuron.calculate_error_outputneuron(outputNN, posible_input[1]) # second parameter is target\n",
    "            \n",
    "                # backpropagation                                      \n",
    "                for left_layer, right_layer in zip(self.neuron_layers[::-1], self.neuron_layers[-2::-1]):\n",
    "                    for neuron in right_layer:\n",
    "                        for weight in enumerate(right_layer.self.neurons.self.weights):\n",
    "                            leftlayer.self.neurons[weight[0]].self.error += weight[1] * right_layer.self.neurons.self.error\n",
    "                            leftlayer.self.neurons[weight[0]].backpropagation()                                                        \n",
    "                                                                             \n",
    "                # update\n",
    "                for layer in self.neuron_layers:\n",
    "                    layer.update_layer()\n",
    "                    #for neuron in self.neuron_layers[layer[0]]:\n",
    "                        #neuron.update()\n",
    "        \n",
    "                # calculate error\n",
    "                li.append(output_NN)\n",
    "        \n",
    "            error = MSE_error([x[1] for x in posible_input], li)\n",
    "            print('error = ', error)\n",
    "            if error < 0.1 and error > - 0.1:\n",
    "                print('network trained!')\n",
    "                print('the weights are', perceptron.weights)\n",
    "                print('the bias is', perceptron.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOR-poort\n",
    "\n",
    "input_neuron1 = Neuron([100, 0, 0], -10)\n",
    "input_neuron2 = Neuron([0, 100, 0], -10)\n",
    "input_neuron3 = Neuron([0, 0, 100], -10)\n",
    "\n",
    "nor_neuron = Neuron([-20, -20, -20], 10)\n",
    "\n",
    "input_layer = Neuron_layer([input_neuron1, input_neuron2, input_neuron3])\n",
    "output_layer = Neuron_layer([nor_neuron])\n",
    "\n",
    "neural_network_NOR = Neuron_network([output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND-poort\n",
    "\n",
    "and_neuron = Neuron([-20, -20], 10)\n",
    "\n",
    "output_layer = Neuron_layer([and_neuron])\n",
    "\n",
    "neural_network_AND = Neuron_network([output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_AND.train_nn([[[0, 0], 0],\n",
    "                             [[0, 1], 0],\n",
    "                             [[1, 0], 0],\n",
    "                             [[1, 1], 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOR_network activate\n",
    "\n",
    "inputs = [[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,0,1],[1,1,0],[1,1,1]]\n",
    "\n",
    "print('x1, x2, NOR_port')\n",
    "for input_x in inputs:\n",
    "    print(input_x, '   ', neural_network_NOR.feed_forward(input_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half adder\n",
    "\n",
    "hidden_layer_X1 = Neuron([100, 0], -10)\n",
    "# Fill up the Neuron weights with zero's equal to that many values you want to put into the Neural Network.\n",
    "# Warning! When you give weights 0 and 1 to neuron_X2 the value will set to only the value of x2. If not this neuron will get the value of x1!\n",
    "hidden_layer_X2 = Neuron([0, 100], -10)\n",
    "\n",
    "hidden_neuron_NAND = Neuron([-60, -60], 100)\n",
    "hidden_neuron_OR = Neuron([20, 20], -10)\n",
    "hidden_neuron_AND = Neuron([30, 30], -40)\n",
    "\n",
    "output_neuron_sum = Neuron([15, 15, 0], -20) # AND-poort\n",
    "output_neuron_carry = Neuron([0, 0, 30], -20) \n",
    "\n",
    "input_neuron_layer = Neuron_layer([hidden_layer_X1, hidden_layer_X2])\n",
    "hidden_neuron_layer = Neuron_layer([hidden_neuron_NAND, hidden_neuron_OR, hidden_neuron_AND])\n",
    "output_neuron_layer = Neuron_layer([output_neuron_carry, output_neuron_sum])\n",
    "\n",
    "neural_network_neurons = Neuron_network([input_neuron_layer, hidden_neuron_layer, output_neuron_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bij sigmoid gaan we er vanuit de het omslagpunt altijd op de nul ligt. Daarom kun je dat omslagpunt niet veranderen. Dan werkt de sigmoid functie niet meer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# half adder activate\n",
    "\n",
    "inputs = [[0,0],[0,1],[1,0],[1,1]]\n",
    "\n",
    "print('x1, x2, carry, sum')\n",
    "for input_x in inputs:\n",
    "    print(input_x, '   ', neural_network_neurons.feed_forward(input_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit6aefb534615b451c8b9f779fc82cec76"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
